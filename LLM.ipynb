{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz0qSOyBxox3"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r5FkwUlS-TU",
        "outputId": "7969f749-d3b0-4b14-f4f0-b3f74ddd7395"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rofh1HStO6-F"
      },
      "source": [
        "埋め込み層"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiPJvz5hO6qu"
      },
      "outputs": [],
      "source": [
        "class PositionEmbedding(nn.Module):\n",
        "    def __init__(self, context_size , d_model):\n",
        "        super(PositionEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(context_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1), device=x.device)\n",
        "        return self.embedding(positions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeaA0-5OBc4H",
        "outputId": "68afcee8-71d8-4786-cb41-fe17ccd60bd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.1679,  1.2869, -0.9309,  ...,  0.4887,  1.3568, -0.7703],\n",
              "        [ 0.2048,  0.1246, -1.9271,  ...,  0.0097, -1.1410,  1.0631],\n",
              "        [ 1.0481,  0.5954,  0.4780,  ...,  0.4574,  0.0223, -0.8736],\n",
              "        [ 1.7000,  0.6803, -0.1919,  ..., -0.1198, -0.5429, -0.6642]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.LongTensor([[4545, 8410, 458, 3]])\n",
        "position_embedding = PositionEmbedding(4, 256)\n",
        "wpe = position_embedding(x)\n",
        "wpe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv-sLG2701k5"
      },
      "source": [
        "Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W85YnsQO3Hon"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, context_size, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(context_size, d_model)\n",
        "\n",
        "        for pos in range(context_size):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos,i]   = math.sin(pos/(10000**((2*i)/d_model)))\n",
        "                pe[pos,i+1] = math.cos(pos/(10000**((2*i)/d_model)))\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)].detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKPZDjq6Ssyb",
        "outputId": "17e040cb-fc41-4e88-8989-4a5552ff3bc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
              "           0.0000e+00,  1.0000e+00],\n",
              "         [ 8.4147e-01,  5.4030e-01,  7.6172e-01,  ...,  1.0000e+00,\n",
              "           1.1548e-08,  1.0000e+00],\n",
              "         [ 9.0930e-01, -4.1615e-01,  9.8705e-01,  ...,  1.0000e+00,\n",
              "           2.3096e-08,  1.0000e+00],\n",
              "         [ 1.4112e-01, -9.8999e-01,  5.1731e-01,  ...,  1.0000e+00,\n",
              "           3.4643e-08,  1.0000e+00]]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.LongTensor([[4545, 8410, 458, 3]]) ##学習\n",
        "positional_encoding = PositionalEncoding(4, 256)\n",
        "wpe = positional_encoding(x)\n",
        "wpe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9pGOSeYn1pYt"
      },
      "outputs": [],
      "source": [
        "def create_attention_mask(context_size):\n",
        "    mask = torch.ones((context_size, context_size))\n",
        "    mask = torch.triu(mask, diagonal=1)\n",
        "    mask = mask == 0\n",
        "    mask = mask * 1\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHOVYS4Imukc",
        "outputId": "6f049818-1766-459e-f0de-87f16f922e5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "create_attention_mask(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOvsG1ozcoGg"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    ''' Scaled Dot-Product Attention '''\n",
        "\n",
        "    def __init__(self, d_model, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.sqrt_d_k = d_model ** 0.5\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        score = torch.matmul(q, k.transpose(2, 3)) /  self.sqrt_d_k\n",
        "\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        attn = F.softmax(score, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.matmul(attn, v)\n",
        "\n",
        "        return output, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZfGHmXSj0Yo",
        "outputId": "a22ba534-375e-4332-e271-a75713e73ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16.0\n",
            "16.0\n"
          ]
        }
      ],
      "source": [
        "print(math.sqrt(256))\n",
        "print(256 ** 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVTsLKp0akNQ",
        "outputId": "2f124652-c631-4bdd-924d-7c2c2915e606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.9215, -0.0370,  1.5726, -0.5920],\n",
            "        [ 1.0876,  0.3483, -1.5761, -1.3926],\n",
            "        [ 0.6543, -0.0932, -2.3670, -1.5800]])\n",
            "tensor([[ 1.4320, -0.1837,  0.5067, -0.5297],\n",
            "        [ 0.6984, -1.6625,  1.1740,  0.2864],\n",
            "        [-1.2627, -1.2706, -1.5757,  0.1019]])\n",
            "tensor([[-0.6995,  2.1653,  0.8612, -0.0959],\n",
            "        [ 1.0172,  0.8018, -1.1667,  2.4077],\n",
            "        [ 0.3414,  0.0369,  0.6523,  1.5866]])\n",
            "tensor([[-0.2024,  1.0945, -1.3278],\n",
            "        [ 1.4324, -2.0687,  0.5257],\n",
            "        [ 0.5916, -2.6194,  2.8608]])\n"
          ]
        }
      ],
      "source": [
        "context_size = 3\n",
        "dim = 4\n",
        "q = torch.randn(context_size, dim)\n",
        "k = torch.randn(context_size, dim)\n",
        "v = torch.randn(context_size, dim)\n",
        "a = q @ k.T\n",
        "print(q)\n",
        "print(k)\n",
        "print(v)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUeKwq0jJr13",
        "outputId": "d0780184-8207-499f-fb31-0da7e30305b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1012,  0.5473, -0.6639],\n",
              "        [ 0.7162, -1.0344,  0.2629],\n",
              "        [ 0.2958, -1.3097,  1.4304]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = a / (dim ** 0.5)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqWU_o7VinX5",
        "outputId": "53c2b2e9-9a1e-490f-c1cf-b7f1cb3aa1a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1012,  0.0000, -0.0000],\n",
              "        [ 0.7162, -1.0344,  0.0000],\n",
              "        [ 0.2958, -1.3097,  1.4304]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask = create_attention_mask(context_size)\n",
        "mask\n",
        "a = a * mask\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7IEgJFXwBOE",
        "outputId": "5daffb83-6aa2-426d-dab1-4bf7937336b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1012,    -inf,    -inf],\n",
              "        [ 0.7162, -1.0344,    -inf],\n",
              "        [ 0.2958, -1.3097,  1.4304]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn = a.masked_fill(mask == 0, float(\"-inf\"))\n",
        "attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKNOk5XCO82F",
        "outputId": "9bd2edb1-cd5f-4380-a803-0d137baae199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000],\n",
              "        [0.8520, 0.1480, 0.0000],\n",
              "        [0.2320, 0.0466, 0.7214]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn = F.softmax(attn, dim=-1)\n",
        "attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zClu0Jw8opQ",
        "outputId": "962142ab-0d04-4583-a69e-9be1ded15674"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 3])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTJ3ADXI8VRh",
        "outputId": "1fbf3398-2cd2-4499-c0bd-7019b271b42d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.6995,  2.1653,  0.8612, -0.0959],\n",
              "        [ 1.0172,  0.8018, -1.1667,  2.4077],\n",
              "        [ 0.3414,  0.0369,  0.6523,  1.5866]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v.shape\n",
        "v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl4Bociy6uwT",
        "outputId": "e2059ad1-0939-4607-b096-31026c771e47"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.6995,  2.1653,  0.8612, -0.0959],\n",
              "        [-0.4454,  1.9635,  0.5611,  0.2746],\n",
              "        [ 0.1314,  0.5663,  0.6160,  1.2345]])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.matmul(attn, v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcyGC6kg1qPt"
      },
      "source": [
        "Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7-h_jDn5xtg"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    ''' Multi-Head Attention module '''\n",
        "\n",
        "    def __init__(self, n_head, d_model, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "        self.fc_q = nn.Linear(d_model, d_model)\n",
        "        self.fc_k = nn.Linear(d_model, d_model)\n",
        "        self.fc_v = nn.Linear(d_model, d_model)\n",
        "        self.attn = ScaledDotProductAttention(d_model, dropout_rate)\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        ## 全結合層を初期化\n",
        "        nn.init.xavier_uniform_(self.fc_q.weight)\n",
        "        nn.init.xavier_uniform_(self.fc_k.weight)\n",
        "        nn.init.xavier_uniform_(self.fc_v.weight)\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        N = q.size(0)\n",
        "        S = q.size(1)\n",
        "        H = self.n_head\n",
        "        D = self.d_model // self.n_head\n",
        "\n",
        "\n",
        "        # 線形変換\n",
        "        q = self.fc_q(q)\n",
        "        k = self.fc_k(k)\n",
        "        v = self.fc_v(v)\n",
        "\n",
        "        # 展開\n",
        "        q = q.view(N, S, H, D)\n",
        "        k = k.view(N, S, H, D)\n",
        "        v = v.view(N, S, H, D)\n",
        "\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "        x, attn = self.attn(q, k, v, mask=mask)\n",
        "        x = x.transpose(1, 2).contiguous().view(N, S, -1)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgo2ZEOtzjOO",
        "outputId": "7862f436-239e-4267-d3e7-e59c98923270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 15, 8, 256])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([128, 8, 15, 256])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.randint(10,(128,15,8,256))\n",
        "print(a.shape)\n",
        "a.transpose(1,2).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5gIUcDEg4Ed",
        "outputId": "84d65a77-6423-4cfd-c279-19c3ddd5568f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultiHeadAttention(\n",
              "  (fc_q): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (fc_k): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (fc_v): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (attn): ScaledDotProductAttention(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (fc): Linear(in_features=16, out_features=16, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_head = 8\n",
        "d_model = 16\n",
        "attention = MultiHeadAttention(n_head, d_model)\n",
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0pq4rImnbmf"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "context_size = 10\n",
        "x = torch.randn(batch_size, context_size, d_model)\n",
        "q, w = attention(x, x, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTbIAwh92yk9",
        "outputId": "78a35a99-4bde-4768-d855-8de6d95f9867"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 16])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq27yoyB2-of",
        "outputId": "f5d9ca52-3687-45d8-f272-55b8f4479b44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 10, 10])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW3puAtb0n81"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, dropout_rate=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(d_model, d_model * 4 )\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(d_model * 4, d_model)\n",
        "\n",
        "        ## 全結合層を初期化\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.fc1(x)\n",
        "        h = F.gelu(h)\n",
        "        h = self.fc2(h)\n",
        "        h = self.dropout(h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ7aagj8tiDI",
        "outputId": "d0fe7813-9bbc-49c9-8469-bfbc2c518b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForward(\n",
            "  (fc1): Linear(in_features=2, out_features=8, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc2): Linear(in_features=8, out_features=2, bias=True)\n",
            ")\n",
            "tensor([-1.2393, -1.7066])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([-0.3460,  0.3750], grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d_model = 2\n",
        "ff = FeedForward(d_model)\n",
        "print(ff)\n",
        "x = torch.randn(d_model)\n",
        "print(x)\n",
        "ff(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZLmeeB200KO"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_head, dropout_rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "        self.attn = MultiHeadAttention(n_head, d_model, dropout_rate)\n",
        "        self.ff = FeedForward(d_model)\n",
        "\n",
        "        nn.init.normal_(self.norm_1.weight, mean=0, std=0.02)\n",
        "        nn.init.normal_(self.norm_2.weight, mean=0, std=0.02)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        rx = x \n",
        "        x, w = self.attn(x, x, x, mask)\n",
        "        x = self.norm_1(x + rx)\n",
        "\n",
        "        rx = x\n",
        "        x = self.ff(x)\n",
        "        x = self.norm_2(x + rx)\n",
        "\n",
        "        return x, w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vctZUrjZxQ-r",
        "outputId": "78b1c5c9-f131-427f-c59e-cb7795b9ad5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.7771,  1.0819],\n",
            "         [ 0.4459, -0.7646],\n",
            "         [ 0.3905,  0.8654],\n",
            "         [-1.0705,  0.7716],\n",
            "         [-0.2005, -0.2011]]])\n",
            "torch.Size([1, 5, 2])\n",
            "tensor([[[ 0.0163, -0.0240],\n",
            "         [ 0.0163, -0.0239],\n",
            "         [ 0.0163, -0.0239],\n",
            "         [ 0.0163, -0.0239],\n",
            "         [ 0.0163, -0.0239]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "torch.Size([1, 1, 5, 5])\n",
            "tensor([[[[0.2312, 0.2105, 0.2130, 0.2356, 0.2208],\n",
            "          [0.2304, 0.2123, 0.1645, 0.2665, 0.2375],\n",
            "          [0.2318, 0.2110, 0.1893, 0.2505, 0.0000],\n",
            "          [0.2310, 0.2109, 0.0000, 0.2353, 0.2209],\n",
            "          [0.2310, 0.2122, 0.1826, 0.2541, 0.2312]]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "d_model = 2\n",
        "n_head = 1\n",
        "block = TransformerBlock(d_model, n_head)\n",
        "batch_size = 1\n",
        "context_size = 5\n",
        "\n",
        "x = torch.randn(batch_size, context_size, d_model)\n",
        "y, w = block(x)\n",
        "print(x)\n",
        "print(y.shape)\n",
        "print(y)\n",
        "print(w.shape)\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H3MZSp4m3A7"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module): ## メイン\n",
        "    def __init__(self, vocab_size, context_size, d_model, n_block, n_head, dropout_rate=0.1):\n",
        "        super(GPT, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.context_size = context_size\n",
        "        self.d_model = d_model\n",
        "        self.n_block = n_block\n",
        "        self.n_head = n_head\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(context_size, d_model)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(d_model, n_head, dropout_rate) for _ in range(self.n_block)])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.fc = nn.Linear(d_model * context_size, vocab_size)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "        nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.token_embedding(x) + self.positional_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x, w = block(x, mask)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = x.view(-1, self.context_size * self.d_model)\n",
        "        x = self.fc(x)\n",
        "\n",
        "\n",
        "        return x, w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J4jwV910y_Ty"
      },
      "outputs": [],
      "source": [
        "context_size = 5\n",
        "vocab_size = 10  \n",
        "d_model = 8\n",
        "n_block = 6\n",
        "n_head = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo8vzm01zSWX"
      },
      "outputs": [],
      "source": [
        "model = GPT(vocab_size, context_size, d_model, n_block, n_head)\n",
        "mask = create_attention_mask(context_size).to(device)\n",
        "x = torch.LongTensor([[2,2,9,4,9]]) # 0～9までの数値を使って context_size の長さの配列を作成します。\n",
        "y, w = model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoE2-f8WTeyU",
        "outputId": "aa25acde-db40-416e-8b5f-4b08f42cce05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.1442, -0.9293, -2.4225, -1.8907,  0.2925, -0.4585, -1.0107, -0.3088,\n",
              "         -1.3548, -1.2617]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FBnCUDhTf4_",
        "outputId": "2e16581f-b382-48f3-d1d1-c263d839c54e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[0.2222, 0.2223, 0.2223, 0.0000, 0.0000],\n",
              "          [0.2222, 0.2223, 0.0000, 0.2222, 0.2222],\n",
              "          [0.2222, 0.2223, 0.2223, 0.0000, 0.2222],\n",
              "          [0.2222, 0.2223, 0.2223, 0.2222, 0.2222],\n",
              "          [0.2222, 0.2223, 0.2223, 0.2222, 0.2222]],\n",
              "\n",
              "         [[0.2222, 0.2222, 0.2222, 0.2222, 0.2222],\n",
              "          [0.2222, 0.2222, 0.2222, 0.2222, 0.2222],\n",
              "          [0.2222, 0.2222, 0.2222, 0.2222, 0.2222],\n",
              "          [0.2222, 0.2222, 0.2222, 0.2222, 0.2222],\n",
              "          [0.2222, 0.2222, 0.0000, 0.2222, 0.2222]],\n",
              "\n",
              "         [[0.2223, 0.2222, 0.2222, 0.2222, 0.2222],\n",
              "          [0.2223, 0.2222, 0.0000, 0.2222, 0.0000],\n",
              "          [0.2223, 0.2222, 0.2222, 0.2222, 0.2222],\n",
              "          [0.2223, 0.2222, 0.2222, 0.2222, 0.0000],\n",
              "          [0.2223, 0.2222, 0.2222, 0.2222, 0.2222]],\n",
              "\n",
              "         [[0.2222, 0.2222, 0.2222, 0.2222, 0.2222],\n",
              "          [0.2222, 0.2222, 0.2222, 0.2222, 0.2222],\n",
              "          [0.2222, 0.2222, 0.2222, 0.0000, 0.2222],\n",
              "          [0.2222, 0.2222, 0.2222, 0.2222, 0.2222],\n",
              "          [0.2222, 0.2222, 0.2222, 0.2222, 0.2222]]]], grad_fn=<MulBackward0>)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
